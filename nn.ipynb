{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "nn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/ml_projects/blob/main/nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfXYLBdbGydw"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUakn7sHdUu"
      },
      "source": [
        "class NeuronModel():\n",
        "    \"\"\"Single Layered Neural Network.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    num_features : int\n",
        "      Number of features of the dataset\n",
        "    weights : 1d-array\n",
        "      Parameters for the features\n",
        "    bias : 1d-array\n",
        "      Parameter for the bias term\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = torch.zeros(num_features, 1, \n",
        "                                   dtype=torch.float)\n",
        "        self.bias = torch.zeros(1, dtype=torch.float)\n",
        "        \n",
        "    def netinput_func(self, x, w, b):\n",
        "        \"\"\"Matrix multiply between weights and features and add the bias term.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        w : {array-like}, shape = [num_features, 1]\n",
        "          Weight parameters, where num_features is the number of features\n",
        "        b : array-like, shape = [1]\n",
        "          Bias parameter\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        {array-like}, shape = [n_examples, 1]\n",
        "          Matrix product of weights and features plus the bias term, where\n",
        "          n_examples is the number of examples \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.add(torch.mm(x, w), b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network. netinput_func computes the matrix\n",
        "        multiply followed by the activation_func which computes tanh activation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        {array-like}, shape = [n_examples, 1]\n",
        "          Activations, where n_examples is the total number of examples. Since \n",
        "          this is a single layered network the activations are also the \n",
        "          predictions\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        netinputs = self.netinput_func(x, self.weights, self.bias)\n",
        "        activations = self.activation_func(netinputs)\n",
        "        return activations.view(-1)\n",
        "        \n",
        "    def backward(self, x, yhat, y):\n",
        "        \"\"\"\n",
        "        Computes gradients (partial derivaties of loss function with respect \n",
        "        to weights and bias)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        yhat : {array-like}, shape = [n_examples, 1]\n",
        "          Activations, where n_examples are number of examples\n",
        "        y : {array-like}, shape = [n_examples, 1]\n",
        "          Target labels, where n_examples are number of examples\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        {array-like}, shape = [n_features, 1]\n",
        "          Negative gradients of the loss with respect to the weight parameters,\n",
        "          where num_features is the number of features\n",
        "\n",
        "        {array-like}, shape = [1]\n",
        "          Negative gradients of the loss with respect to the bias parameter\n",
        "\n",
        "        \"\"\"  \n",
        "        \n",
        "        netinputs = self.netinput_func(x, self.weights, self.bias)\n",
        "        \n",
        "        \n",
        "        grad_loss_yhat = 2 * (yhat - y)         # x[0] = (10,)                                                      # partial derivative of the loss with respect to activation\n",
        "\n",
        "\n",
        "        # grad_yhat_bias = (4 / (torch.exp(netinputs) \\\n",
        "        # + torch.exp(-netinputs))**2)            # dim of n                                   ## alternate method of computing the derivative.\n",
        "\n",
        "        # grad_yhat_weights = (4 / (torch.exp(netinputs) \\                                    ## alternate method of computing the derivative.  \n",
        "        # + torch.exp(-netinputs))**2) * x     # dim of x\n",
        "\n",
        "        grad_yhat_weights = (1 - self.activation_func(netinputs)**2) * x                      #dim of x (10X2)            partial derivative of activation with respect to weights \n",
        "        grad_yhat_bias = (1 - self.activation_func(netinputs)**2)                             #dim of x[0] (10,)          partial derivative of activation with respect to bias\n",
        "        \n",
        "        grad_loss_weights = torch.mm(grad_yhat_weights.t(), \n",
        "                                     grad_loss_yhat.view(-1, 1)) / y.size(0)                  #dim = (2X10).dot(10X1) = 2X1       Using the chain rule (inner and outer)\n",
        "        grad_loss_bias = torch.sum(grad_loss_yhat * grad_yhat_bias) / y.size(0)               #dim = sum((10)*(10)) = 1          Using the chain rule (inner and outer)\n",
        "\n",
        "        \n",
        "        return (-1)*grad_loss_weights, (-1)*grad_loss_bias\n",
        "\n",
        "    def activation_func(self, x):\n",
        "        \"\"\"Calculates tanh activation function\"\"\"\n",
        "        return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "\n",
        "    def loss(self, yhat, y):\n",
        "        \"\"\"\n",
        "        Returns mean squared error\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.mean((yhat - y)**2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ4RFLSGGKB"
      },
      "source": [
        "model = NeuronModel(num_features=2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i27Gd-SHBHP"
      },
      "source": [
        "X = torch.randn((100, 2))\n",
        "y = torch.tensor([0]*50 + [1]*50)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZKYmfxbHaRA"
      },
      "source": [
        "y_hat = model.forward(X)\n",
        "weights, bias = model.backward(X, y_hat, y)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0KJh9_gHtwx",
        "outputId": "dec1c4a7-bcf6-496c-b93e-11caae86da7d"
      },
      "source": [
        "print(f\"For one iteration the gradient of loss with respect to weights is {weights} and bias is {bias}.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For one iteration the gradient of loss with respect to weights is tensor([[ 0.0921],\n",
            "        [-0.1037]]) and bias is 100.0.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}