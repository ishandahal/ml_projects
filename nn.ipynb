{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "nn.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/ml_projects/blob/main/nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUakn7sHdUu"
      },
      "source": [
        "class NeuronModel():\n",
        "    \"\"\" \n",
        "    Single layered neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        self.num_features = num_features\n",
        "        self.weights = torch.zeros(num_features, 1, \n",
        "                                   dtype=torch.float)\n",
        "        self.bias = torch.zeros(1, dtype=torch.float)\n",
        "        \n",
        "    def activation_func(self, x):\n",
        "        \"\"\" \n",
        "        Calculates tanh activation function\n",
        "        \"\"\"\n",
        "\n",
        "        return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "    \n",
        "    def netinput_func(self, x, w, b):\n",
        "        \"\"\"\n",
        "        Multliplies inputs with weights plus bias term\n",
        "        \"\"\"\n",
        "\n",
        "         return torch.add(torch.mm(x, w), b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \"\"\"\n",
        "\n",
        "        netinputs = self.netinput_func(x, self.weights, self.bias)\n",
        "        activations = self.activation_func(netinputs)\n",
        "        return activations.view(-1)\n",
        "        \n",
        "    def backward(self, x, yhat, y):\n",
        "        \"\"\"\n",
        "        Calculates gradients (partial derivaties of loss function with respect to weights and bias)\n",
        "        \"\"\"  \n",
        "        \n",
        "        # note that here, \"yhat\" are the \"activations\" \n",
        "        netinputs = self.netinput_func(x, self.weights, self.bias)\n",
        "        \n",
        "        \n",
        "        grad_loss_yhat = 2 * (yhat - y)         # x[0] = (10,)          # partial derivaitve of the loss with respect to activation\n",
        "\n",
        "\n",
        "        # grad_yhat_bias = (4 / (torch.exp(netinputs) + torch.exp(-netinputs))**2)            # dim of n            ## different method of coming to the derivative.\n",
        "        # grad_yhat_weights = (4 / (torch.exp(netinputs) + torch.exp(-netinputs))**2) * x     # dim of x\n",
        "\n",
        "        grad_yhat_weights = (1 - self.activation_func(netinputs)**2) * x        #dim of x (10X2)            partial derivative of activation with respect to weights \n",
        "        grad_yhat_bias = (1 - self.activation_func(netinputs)**2)               #dim of x[0] (10,)          partial derivative of activation with respect to bias\n",
        "        \n",
        "        grad_loss_weights = torch.mm(grad_yhat_weights.t(), \n",
        "                                     grad_loss_yhat.view(-1, 1)) / y.size(0)     #dim = (2X10).dot(10X1) = 2X1       Using the chain rule (inner and outer)\n",
        "        grad_loss_bias = torch.sum(grad_loss_yhat * grad_yhat_bias) / y.size(0)  #dim = sum((10)*(10)) = 1          Using the chain rule (inner and outer)\n",
        "\n",
        "        \n",
        "        return (-1)*grad_loss_weights, (-1)*grad_loss_bias\n",
        "\n",
        "def loss(yhat, y):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.mean((yhat - y)**2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}